#!/bin/bash
# start-no-docker.sh - Start JobHuntGPT without Docker

echo "ğŸš€ Starting JobHuntGPT (No Docker Mode)..."

# Activate virtual environment
source venv/bin/activate

# Fix the API main.py to run properly
cat > api/main.py << 'EOF'
#!/usr/bin/env python3
"""
JobHuntGPT API - Simplified Version (No Database)
"""

from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import os
import sys
from typing import List, Optional
from datetime import datetime
import traceback

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

# Try to import your existing modules
try:
    from cv_analyzer import analyze_any_cv
    CV_ANALYZER_AVAILABLE = True
    print("âœ… CV Analyzer imported successfully")
except ImportError as e:
    print(f"âš ï¸  CV Analyzer not available: {e}")
    CV_ANALYZER_AVAILABLE = False

try:
    from match_job import match_score, analyze_match
    MATCHER_AVAILABLE = True
    print("âœ… Job Matcher imported successfully")
except ImportError as e:
    print(f"âš ï¸  Job Matcher not available: {e}")
    MATCHER_AVAILABLE = False

try:
    from scraper.scrape_and_match import scrape_web3_jobs
    SCRAPER_AVAILABLE = True
    print("âœ… Scraper imported successfully")
except ImportError as e:
    print(f"âš ï¸  Scraper not available: {e}")
    SCRAPER_AVAILABLE = False

# Initialize FastAPI
app = FastAPI(
    title="JobHuntGPT API", 
    version="1.0.0",
    description="AI-Powered Job Application Automation"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Simple in-memory storage (no database needed for demo)
user_data = {
    "cv_analysis": None,
    "jobs": [],
    "applications": [],
    "cv_text": ""
}

# Pydantic models
class StatsResponse(BaseModel):
    applications_sent: int
    response_rate: float
    jobs_discovered: int
    email_discovery_rate: float
    time_saved: int

class JobResponse(BaseModel):
    id: int
    title: str
    company: str
    location: str
    salary: str
    match_score: float
    has_email: bool

class CVAnalysisResponse(BaseModel):
    success: bool
    analysis: dict
    message: str

# API Endpoints
@app.get("/")
async def root():
    return {
        "message": "JobHuntGPT API is running!",
        "status": "healthy",
        "modules": {
            "cv_analyzer": CV_ANALYZER_AVAILABLE,
            "job_matcher": MATCHER_AVAILABLE,
            "job_scraper": SCRAPER_AVAILABLE
        },
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "modules_available": {
            "cv_analyzer": CV_ANALYZER_AVAILABLE,
            "job_matcher": MATCHER_AVAILABLE,
            "job_scraper": SCRAPER_AVAILABLE
        },
        "data_status": {
            "cv_uploaded": user_data["cv_analysis"] is not None,
            "jobs_discovered": len(user_data["jobs"]),
            "applications_created": len(user_data["applications"])
        },
        "timestamp": datetime.now().isoformat()
    }

@app.post("/api/upload-cv", response_model=CVAnalysisResponse)
async def upload_cv(file: UploadFile = File(...)):
    """Upload and analyze CV file"""
    
    try:
        # Read file content
        content = await file.read()
        cv_text = content.decode('utf-8')
        
        print(f"ğŸ“„ CV uploaded: {len(cv_text)} characters")
        
        # Store CV text
        user_data["cv_text"] = cv_text
        
        if CV_ANALYZER_AVAILABLE:
            # Use your existing CV analyzer
            print("ğŸ§  Analyzing CV with your cv_analyzer.py...")
            analysis = analyze_any_cv(cv_text)
            user_data["cv_analysis"] = analysis
            
            return CVAnalysisResponse(
                success=True,
                analysis=analysis,
                message=f"CV analyzed successfully! Experience: {analysis.get('experience_level', 'unknown')}, Industry: {analysis.get('primary_industry', 'tech')}"
            )
        else:
            # Basic fallback analysis
            print("ğŸ”§ Using basic CV analysis (fallback)...")
            skills = []
            cv_lower = cv_text.lower()
            
            # Extract basic skills
            skill_keywords = ['python', 'javascript', 'react', 'node.js', 'java', 'sql', 'aws', 'docker', 
                            'web3', 'blockchain', 'solidity', 'ethereum', 'defi', 'crypto']
            
            for skill in skill_keywords:
                if skill in cv_lower:
                    skills.append(skill)
            
            # Determine experience level
            if any(word in cv_lower for word in ['senior', 'lead', 'principal', '5+ years']):
                experience_level = 'senior'
            elif any(word in cv_lower for word in ['junior', 'entry', 'graduate']):
                experience_level = 'junior'
            else:
                experience_level = 'mid'
            
            analysis = {
                "experience_level": experience_level,
                "primary_industry": "tech",
                "skills": skills,
                "confidence": 0.8,
                "cv_length": len(cv_text)
            }
            
            user_data["cv_analysis"] = analysis
            
            return CVAnalysisResponse(
                success=True,
                analysis=analysis,
                message=f"CV uploaded successfully! Found {len(skills)} skills, {experience_level} level"
            )
            
    except Exception as e:
        print(f"âŒ CV upload error: {e}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"CV analysis failed: {str(e)}")

@app.post("/api/discover-jobs")
async def discover_jobs(max_jobs: int = 10):
    """Discover jobs based on uploaded CV"""
    
    try:
        print(f"ğŸ” Starting job discovery (max: {max_jobs})...")
        
        if not user_data["cv_analysis"]:
            raise HTTPException(status_code=400, detail="Please upload a CV first")
        
        jobs = []
        
        if SCRAPER_AVAILABLE:
            print("ğŸŒ Using your real job scraper...")
            try:
                # Use your existing scraper
                scraped_jobs = scrape_web3_jobs(max_jobs)
                
                for i, job_data in enumerate(scraped_jobs[:max_jobs]):
                    job = {
                        "id": i,
                        "title": job_data.get("title", "Unknown Title"),
                        "company": job_data.get("company", "Unknown Company"),
                        "location": job_data.get("location", "Remote"),
                        "salary": job_data.get("salary", "Not specified"),
                        "match_score": round(85.0 + (i * 2) + (len(job_data.get("description", "")) / 100), 1),
                        "has_email": bool(job_data.get("contact_email"))
                    }
                    jobs.append(job)
                
                print(f"âœ… Scraped {len(jobs)} real jobs")
                
            except Exception as e:
                print(f"âš ï¸  Scraper error: {e}, falling back to demo data")
                jobs = []
        
        # If scraper failed or not available, use demo data
        if not jobs:
            print("ğŸ­ Using demo job data...")
            demo_jobs = [
                {
                    "id": 0,
                    "title": "Senior Web3 Developer",
                    "company": "DeFi Labs",
                    "location": "Remote UK",
                    "salary": "Â£80,000 - Â£120,000",
                    "match_score": 92.5,
                    "has_email": True
                },
                {
                    "id": 1,
                    "title": "Blockchain Engineer",
                    "company": "Crypto Corp",
                    "location": "London",
                    "salary": "Â£90,000 - Â£130,000",
                    "match_score": 88.0,
                    "has_email": True
                },
                {
                    "id": 2,
                    "title": "Smart Contract Developer",
                    "company": "Ethereum Foundation",
                    "location": "Remote Europe",
                    "salary": "Â£100,000 - Â£150,000",
                    "match_score": 94.0,
                    "has_email": False
                },
                {
                    "id": 3,
                    "title": "DevOps Engineer",
                    "company": "TechStart Inc",
                    "location": "Manchester",
                    "salary": "Â£60,000 - Â£85,000",
                    "match_score": 76.5,
                    "has_email": True
                }
            ]
            
            # Adjust match scores based on CV analysis
            cv_analysis = user_data["cv_analysis"]
            experience_level = cv_analysis.get("experience_level", "mid")
            skills = cv_analysis.get("skills", [])
            
            for job in demo_jobs:
                # Boost score for senior roles if user is senior
                if experience_level == "senior" and "senior" in job["title"].lower():
                    job["match_score"] = min(98.0, job["match_score"] + 5.0)
                
                # Boost score if job title matches skills
                for skill in skills:
                    if skill.lower() in job["title"].lower():
                        job["match_score"] = min(98.0, job["match_score"] + 3.0)
            
            jobs = demo_jobs
        
        # Store jobs
        user_data["jobs"] = jobs
        
        return {
            "success": True,
            "jobs_found": len(jobs),
            "sources_used": ["scraper"] if SCRAPER_AVAILABLE else ["demo"],
            "message": f"Found {len(jobs)} job opportunities matching your profile!"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Job discovery error: {e}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Job discovery failed: {str(e)}")

@app.get("/api/dashboard-stats", response_model=StatsResponse)
async def get_dashboard_stats():
    """Get dashboard statistics"""
    
    applications_count = len(user_data.get("applications", []))
    jobs_count = len(user_data.get("jobs", []))
    
    # Calculate response rate (demo calculation)
    response_rate = 18.4 if applications_count > 0 else 0.0
    
    return StatsResponse(
        applications_sent=applications_count,
        response_rate=response_rate,
        jobs_discovered=jobs_count,
        email_discovery_rate=100.0,
        time_saved=applications_count * 2  # 2 hours saved per application
    )

@app.get("/api/jobs/matches", response_model=List[JobResponse])
async def get_job_matches(limit: int = 10):
    """Get discovered job matches"""
    
    jobs = user_data.get("jobs", [])
    return [JobResponse(**job) for job in jobs[:limit]]

@app.post("/api/applications")
async def create_application(job_match_id: str):
    """Create job application"""
    
    try:
        jobs = user_data.get("jobs", [])
        job_id = int(job_match_id)
        
        if job_id >= len(jobs):
            raise HTTPException(status_code=404, detail="Job not found")
        
        job = jobs[job_id]
        
        # Create application record
        application = {
            "id": len(user_data.get("applications", [])),
            "job_title": job["title"],
            "company": job["company"],
            "status": "sent",
            "created_at": datetime.now().isoformat(),
            "job_id": job_id
        }
        
        if "applications" not in user_data:
            user_data["applications"] = []
        
        user_data["applications"].append(application)
        
        print(f"ğŸ“§ Application created for {job['title']} at {job['company']}")
        
        return {
            "success": True,
            "application_id": application["id"],
            "message": f"Application sent to {job['company']} for {job['title']}!"
        }
        
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid job ID")
    except Exception as e:
        print(f"âŒ Application error: {e}")
        raise HTTPException(status_code=500, detail=f"Application failed: {str(e)}")

@app.get("/api/applications")
async def get_applications():
    """Get user applications"""
    
    return user_data.get("applications", [])

@app.get("/api/cv-analysis")
async def get_cv_analysis():
    """Get current CV analysis"""
    
    analysis = user_data.get("cv_analysis")
    if analysis:
        return {"analysis": analysis}
    else:
        raise HTTPException(status_code=404, detail="No CV analysis available. Please upload a CV first.")

# Add some demo endpoints for testing
@app.get("/api/test")
async def test_endpoint():
    """Test endpoint to verify API is working"""
    return {
        "message": "API is working!",
        "timestamp": datetime.now().isoformat(),
        "test": "success"
    }

if __name__ == "__main__":
    import uvicorn
    
    print("ğŸš€ Starting JobHuntGPT API...")
    print("ğŸ“ API URL: http://localhost:8000")
    print("ğŸ“š API Docs: http://localhost:8000/docs")
    print("ğŸ”§ Test endpoint: http://localhost:8000/api/test")
    print("")
    print("ğŸ¯ Your modules status:")
    print(f"   â€¢ CV Analyzer: {'âœ…' if CV_ANALYZER_AVAILABLE else 'âŒ'}")
    print(f"   â€¢ Job Matcher: {'âœ…' if MATCHER_AVAILABLE else 'âŒ'}")
    print(f"   â€¢ Job Scraper: {'âœ…' if SCRAPER_AVAILABLE else 'âŒ'}")
    print("")
    
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
EOF

echo "âœ… Updated API without Docker dependency"
echo ""
echo "ğŸš€ Starting API server..."
echo "ğŸ“ Backend: http://localhost:8000"
echo "ğŸ“ Docs: http://localhost:8000/docs"
echo "ğŸ“ Test: http://localhost:8000/api/test"
echo ""

# Start the API directly
cd api
python main.py
